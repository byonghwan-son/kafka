# 상태가 없는 처리

* 레코드 필터링
* 필드 추가 및 삭제
* 레코드의 키 변경(rekeying)
* 스트림 가지치기(branching)
* 스트림 병합(merging)
* 레코드를 하나 또는 그 이상의 출력으로 변환
* 레코드를 한 번에 하나씩 보강(enrich)

## 상태가 없는 처리 vs 상태가 있는 처리

* 상태가 없는 애플리케이션에서 카프카 스트림즈 애플리케이션이 처리한 각 이벤트는 다른 이벤트와 독립적으로 처리. 애플리케이션은 각 이벤트를 독립적인 삽입(INSERT)으로 간주하고 이전에 처리한 이벤트에 대해서는 어떠한 기억도 안함.
* 상태가 있는 애플리케이션에서는 프로세서 토폴로지의 한 두 단계 이상의 이전 단계에서 처리했던 이벤트에 대한 정보를 기억. 상태가 있는 처리는 보통 집계, 윈도잉 또는 이벤트 스트림들을 조인할 목적으로 사용. 내부적인 처리가 훨씬 복잡함.

filter 연산자 : 무상태  
count 연산자 : 상태  

### 튜토리얼 소개 : 트위터 스트림 처리

![트윗을 보강하는 애플리케이션이 구현할 토폴로지](../img/2023-11-12%2013%2034%2007.png)

### 프로젝트 설치

[git 참조](https://github.com/mitch-seymour/mastering-kafka-streams-and-ksqldb/tree/master/chapter-03/crypto-sentiment)

### KStream 소스 프로세서 추가

하나 이상의 소스 토픽으로부터 데이터를 소비함  
[tweets]라는 토픽  
트위터 소스 커넥터를 이용해 트위터의 Streaming API를 사용해 트윗을 읽고 JSON으로 인코딩.

```java
StreamBuilder builder = new StreamBuilder();

// byte[] 타입의 매개변수 : 빠른 처리 속도
// Key : byte[], Value : byte[]
KStream<byte[], byte[]> stream = builder.stream("tweets");
```

카프카 스트림즈 애플리케이션을 포함해 카프카 클라이언트가 상위 수준의 객체와 문자열, JSON, Avro, ProtoBuf등과 같은 포맷으로 작업하려면 이 바이트 스트림을 직렬과하고 역직렬화하는 책임을 클라이언트가 갖고 있다는 것을 의미

카프카 스트림즈 토폴로지 구축을 실제 애플리케이션 실행 코드와 분리

### 직렬화 / 역직렬화

```java
// CryptoTopology.java
package com.example;

import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Printed;

class CryptoTopology {
    public static Topology build() {
        StreamBuilder builder = new StreamBuilder();

        KStream<byte[], byte[]> stream = builder.stream("tweets");
        stream.print(Printed.<byte[], byte[]>toSysOut().withLabel("tweets-stream"));

        return builder.build();
    }
}
```

```java
/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package com.example;

import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
// import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.Topology;

public class App {
    public static void main(String[] args) {
        Topology topology = CryptoTopology.build();

        Properties config = new Properties();
        config.put(StreamsConfig.APPLICATION_ID_CONFIG, "dev");
        config.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:29092");
        config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        // config.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.Void().getClass());
        // config.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

        KafkaStreams streams = new KafkaStreams(topology, config);

        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));

        System.out.println("Starting Twitter streams...");

        streams.start();
    }
}
```

![직렬화와 역직렬화의 차이](../img/2023-11-13%2020%2029%2031.png)

#### 커스텀 Serdes 만들기

Gson(<https://github.com/google/gson>)

bytes 배열을 객체로 변환  

```java
Gson gson = new Gson();
byte[] bytes = ...;
Type type = ...;
gson.fromJson(new String(bytes), type);
```

객체를 bytes 배열로 변환

```java
Gson gson = new Gson();
gson.toJson(instance).getBytes(StarndardCharsets.UTF_8);
```

```gradle
// settings.gradle
pluginManagement {
    repositories {
        gradlePluginPortal()
        mavenCentral()
    }
}
```

```gradle
// build.gradle
plugins {
    id "com.github.davidmc24.gradle.plugin.avro" version "1.9.1"
}

repositories {
    mavenCentral()
}

dependencies {
    implementation 'org.apache.avro:avro:1.11.0'
}
```

build 디렉토리에 generated-main-avro-java/com/example/model/EntitySentiment.java  
파일이 생성되어 있음.  

gradlew build 할 때에도 자동 생성된 클래스를 참조하도록 import만 해 주면 됨.  

CryptoTopology 클래스에서 아래 이미지의 흐름을 순서대로 호출해서 정의한다.  
소스는 책과 깃허브를 참조해서 작성했다.  

```java
package com.example;

import java.util.Arrays;
import java.util.List;

import com.example.serialization.Tweet;
import com.example.serialization.avro.AvroSerdes;
import com.example.serialization.json.TweetSerdes;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.*;

import com.example.language.LanguageClient;
import com.example.model.EntitySentiment;


public class CryptoTopology {
    private static final List<String> currencies = Arrays.asList("bitcoin", "ethereum");

    // 초기 테스트용(sink가 없는 상태)
    public static Topology build() {
        StreamsBuilder builder = new StreamsBuilder();
    
        KStream<byte[], byte[]> stream = builder.stream("tweets");
        stream.print(Printed.<byte[], byte[]>toSysOut().withLabel("tweets-stream"));
    
        return builder.build();
    }

    // 싱크가 만들어진 상태(모든 흐름을 담고 있음)
    public static Topology build(LanguageClient languageClient) {
        return build(languageClient, true);
    }

    public static Topology build(LanguageClient languageClient, boolean useSchemaRegistry) {
        StreamsBuilder builder = new StreamsBuilder();

        // KStream<byte[], byte[]> stream = builder.stream("tweets");
        // stream.print(Printed.<byte[], byte[]>toSysOut().withLabel("tweets-stream"));

        KStream<byte[], Tweet> stream = builder.stream("tweets", Consumed.with(Serdes.ByteArray(), new TweetSerdes()));
        stream.print(Printed.<byte[], Tweet>toSysOut().withLabel("tweets-stream"));

// filter out retweets
        KStream<byte[], Tweet> filtered =
            stream.filterNot(
                (key, tweet) -> {
                return tweet.isRetweet();
                });

        // match all tweets that specify English as the source language
        Predicate<byte[], Tweet> englishTweets = (key, tweet) -> tweet.getLang().equals("en");

        // match all other tweets
        Predicate<byte[], Tweet> nonEnglishTweets = (key, tweet) -> !tweet.getLang().equals("en");

        // branch based on tweet language
        KStream<byte[], Tweet>[] branches = filtered.branch(englishTweets, nonEnglishTweets);

        // English tweets
        KStream<byte[], Tweet> englishStream = branches[0];
        englishStream.print(Printed.<byte[], Tweet>toSysOut().withLabel("tweets-english"));

        // non-English tweets
        KStream<byte[], Tweet> nonEnglishStream = branches[1];
        nonEnglishStream.print(Printed.<byte[], Tweet>toSysOut().withLabel("tweets-non-english"));

        // for non-English tweets, translate the tweet text first.
        KStream<byte[], Tweet> translatedStream =
            nonEnglishStream.mapValues(
                (tweet) -> {
                return languageClient.translate(tweet, "en");
                });

        // merge the two streams
        KStream<byte[], Tweet> merged = englishStream.merge(translatedStream);

        // enrich with sentiment and salience scores
        // note: the EntitySentiment class is auto-generated from the schema
        // definition in src/main/avro/entity_sentiment.avsc
        KStream<byte[], EntitySentiment> enriched =
            merged.flatMapValues(
                (tweet) -> {
                // perform entity-level sentiment analysis
                List<EntitySentiment> results = languageClient.getEntitySentiment(tweet);

                // remove all entity results that don't match a currency
                results.removeIf(
                    entitySentiment -> !currencies.contains(entitySentiment.getEntity()));

                return results;
                });

        if (useSchemaRegistry) {
            enriched.to(
                    "crypto-sentiment",
                    // registry-aware Avro Serde
                    Produced.with(
                            Serdes.ByteArray(), AvroSerdes.EntitySentiment("http://localhost:8081", false)));
        } else {
            enriched.to(
                    "crypto-sentiment",
                    Produced.with(
                            Serdes.ByteArray(),
                            // registryless Avro Serde
                            com.mitchseymour.kafka.serialization.avro.AvroSerdes.get(EntitySentiment.class)));
        }

        return builder.build();
    }
}
```

![트윗을 보강하는 애플리케이션이 구현할 토폴로지](../img/2023-11-12%2013%2034%2007.png)

```bash
$ gradlew run --info
# tweets 토픽으로 메세지를 보내면 아래와 같이 표시된다.
[tweets-stream]: null, com.example.serialization.Tweet@256c95f5
[tweets-english]: null, com.example.serialization.Tweet@256c95f5
[tweets-stream]: null, com.example.serialization.Tweet@25008484
```

kafka 도커 내부에서의 결과 확인  

```bash
# 스키마-레지스트리 서버에 접속
$ docker-compose exec schema-registry bash

$ kafka-avro-console-consumer --bootstrap-server kafka:9092 --topic crypto-sentiment --from-beginning
# ...
# 실제로는 아래와 같이 표시가 안되고 긴 문자열로 표시가 됨.
#{"created_at":1577933872630,"id":10005,"entity":"bitcoin","text":"Bitcoin has a lot of promise. I'm not too sure about #ethereum","sentiment_score":0.7203457395209237,"sentiment_magnitude":0.12457374706054891,"salience":0.4758264174842004}
#{"created_at":1577933872630,"id":10005,"entity":"ethereum","text":"Bitcoin has a lot of promise. I'm not too sure about #ethereum","sentiment_score":0.8006875085180427,"sentiment_magnitude":0.9358028525778888,"salience":0.37409993220562}
# 클래스에서 정의한 값만 추출해서 가져 온다.
{
    "created_at": 1577933872630,
    "id": 10005,
    "entity": "bitcoin",
    "text": "Bitcoin has a lot of promise. I'm not too sure about #ethereum",
    "sentiment_score": 0.7203457395209237,
    "sentiment_magnitude": 0.12457374706054891,
    "salience": 0.4758264174842004
},
{
    "created_at": 1577933872630,
    "id": 10005,
    "entity": "ethereum",
    "text": "Bitcoin has a lot of promise. I'm not too sure about #ethereum",
    "sentiment_score": 0.8006875085180427,
    "sentiment_magnitude": 0.9358028525778888,
    "salience": 0.37409993220562
}
```
